{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e4b471",
   "metadata": {},
   "source": [
    "# Document Ingestion and Storage Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471001ce",
   "metadata": {},
   "source": [
    "- Use either a local file path or a URL (not both).\n",
    "- Supported formats by Docling (common): PDF, DOCX, PPTX, XLSX, HTML, TXT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fa7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Input (choose file OR URL)\n",
    "# Supported formats (Docling): PDF, DOCX, PPTX, XLSX, HTML, TXT\n",
    "\n",
    "# Exactly one of these can be set (str). Leave the other as None.\n",
    "# If using the UI below, you can leave both as None here.\n",
    "input_file_path = \"/Users/dimopc/Downloads/startup_technical_guide_ai_agents_final.pdf\"  # e.g., \"/path/to/document.pdf\"\n",
    "input_url = None        # e.g., \"https://example.com/document.pdf\"\n",
    "\n",
    "print({\"file\": input_file_path, \"url\": input_url})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f426c5",
   "metadata": {},
   "source": [
    "### Optional UI — Upload or URL\n",
    "Use this to set input without editing variables in Step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72835d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI widgets: choose File or URL, then click Apply\n",
    "from ipywidgets import FileUpload, Text, ToggleButtons, Button, HBox, VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "mode = ToggleButtons(options=[\"File\", \"URL\"], value=\"File\")\n",
    "uploader = FileUpload(accept=\"\", multiple=False)\n",
    "url_text = Text(placeholder=\"https://example.com/document.pdf\", value=\"\")\n",
    "apply_btn = Button(description=\"Use Selection\", button_style=\"primary\")\n",
    "out = Output()\n",
    "\n",
    "# Globals consumed by Step 2\n",
    "uploaded_bytes = None\n",
    "\n",
    "@apply_btn.on_click\n",
    "def _apply(_):\n",
    "    global input_file_path, input_url, uploaded_bytes\n",
    "    input_file_path = None\n",
    "    input_url = None\n",
    "    uploaded_bytes = None\n",
    "    if mode.value == \"File\":\n",
    "        if uploader.value:\n",
    "            # Take first file only\n",
    "            file_info = next(iter(uploader.value.values()))\n",
    "            uploaded_bytes = file_info.get(\"content\")\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                print(f\"Selected file: {file_info.get('metadata', {}).get('name', 'uploaded')} ({len(uploaded_bytes)} bytes)\")\n",
    "        else:\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                print(\"No file selected.\")\n",
    "    else:\n",
    "        if url_text.value.strip():\n",
    "            input_url = url_text.value.strip()\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                print(f\"Selected URL: {input_url}\")\n",
    "        else:\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                print(\"No URL provided.\")\n",
    "\n",
    "ui = VBox([\n",
    "    mode,\n",
    "    HBox([uploader]),\n",
    "    HBox([url_text]),\n",
    "    apply_btn,\n",
    "    out,\n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Process with Docling (HTTP v1 async) and extract Markdown\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "DOCLING_BASE_URL = os.getenv(\"DOCLING_BASE_URL\", \"http://localhost:5001\")\n",
    "markdown_content = None\n",
    "\n",
    "# Determine input source: exactly one among uploaded_bytes, input_file_path, input_url\n",
    "has_upload = 'uploaded_bytes' in globals() and uploaded_bytes is not None\n",
    "has_path = bool(input_file_path)\n",
    "has_url = bool(input_url)\n",
    "assert (has_upload + has_path + has_url) == 1, \"Provide exactly one source (upload OR file path OR URL).\"\n",
    "\n",
    "# Build sources for Docling Serve v1\n",
    "sources = []\n",
    "if has_url:\n",
    "    sources.append({\"kind\": \"http\", \"url\": input_url})\n",
    "else:\n",
    "    if has_path:\n",
    "        with open(input_file_path, \"rb\") as f:\n",
    "            b = f.read()\n",
    "        filename = os.path.basename(input_file_path)\n",
    "    else:  # has_upload\n",
    "        b = uploaded_bytes\n",
    "        filename = \"upload.bin\"\n",
    "    sources.append({\n",
    "        \"kind\": \"file\",\n",
    "        \"base64_string\": base64.b64encode(b).decode(\"ascii\"),\n",
    "        \"filename\": filename,\n",
    "    })\n",
    "\n",
    "# Submit async job\n",
    "submit_payload = {\n",
    "    \"sources\": sources,\n",
    "    \"output_formats\": [\"md\"],             # accepted by v1 sync API\n",
    "    \"options\": {\n",
    "        \"to_formats\": [\"md\"],\n",
    "        \"include_images\": False,\n",
    "        \"do_picture_description\": False,\n",
    "        \"do_picture_classification\": False,\n",
    "        \"do_formula_enrichment\": False,\n",
    "        \"do_code_enrichment\": False,\n",
    "        \"image_export_mode\": \"placeholder\"\n",
    "    },\n",
    "}\n",
    "resp = requests.post(\n",
    "    f\"{DOCLING_BASE_URL}/v1/convert/source/async\",\n",
    "    json=submit_payload,\n",
    "    timeout=30,\n",
    ")\n",
    "resp.raise_for_status()\n",
    "job = resp.json()\n",
    "\n",
    "# Poll status until success/failure (max ~5 min)\n",
    "max_wait_s = 300\n",
    "poll_interval_s = 2\n",
    "start = time.time()\n",
    "status = job.get(\"task_status\", \"pending\")\n",
    "task_id = job[\"task_id\"]\n",
    "while status not in (\"success\", \"failure\"):\n",
    "    if time.time() - start > max_wait_s:\n",
    "        raise TimeoutError(f\"Docling async job timed out after {max_wait_s}s (task_id={task_id}).\")\n",
    "    s = requests.get(f\"{DOCLING_BASE_URL}/v1/status/poll/{task_id}\", timeout=15)\n",
    "    s.raise_for_status()\n",
    "    job = s.json()\n",
    "    status = job.get(\"task_status\", \"pending\")\n",
    "    time.sleep(poll_interval_s)\n",
    "\n",
    "if status != \"success\":\n",
    "    raise RuntimeError(f\"Docling job failed (task_id={task_id}). Details: {job}\")\n",
    "\n",
    "# Fetch result\n",
    "r = requests.get(f\"{DOCLING_BASE_URL}/v1/result/{task_id}\", timeout=120)\n",
    "r.raise_for_status()\n",
    "res = r.json() if r.headers.get(\"content-type\", \"\").startswith(\"application/json\") else {}\n",
    "\n",
    "# Prefer 'document.md_content' if provided; fallback to previous map style\n",
    "doc_obj = res.get(\"document\") or {}\n",
    "markdown_content = doc_obj.get(\"md_content\")\n",
    "\n",
    "if not markdown_content:\n",
    "    output = res.get(\"output\") or {}\n",
    "    md_key = next((k for k in output.keys() if k.endswith(\".md\")), None)\n",
    "    markdown_content = output.get(md_key, \"\") if md_key else output.get(\"markdown\", \"\")\n",
    "\n",
    "print(\"--- Extracted Markdown (preview) ---\")\n",
    "print(markdown_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66812bed",
   "metadata": {},
   "source": [
    "### Step 3 — Hybrid chunking via Docling Serve (merge_peers)\n",
    "Uses the HTTP hybrid chunker with ~384-token chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Chunk using Docling Serve hybrid chunker (≈384 tokens, merge_peers)\n",
    "import base64\n",
    "import os\n",
    "import requests\n",
    "\n",
    "TARGET_TOKENS = 384  # all-MiniLM-L6-v2 embedding dimension\n",
    "\n",
    "# Build sources again (same logic as Step 2)\n",
    "has_upload = 'uploaded_bytes' in globals() and uploaded_bytes is not None\n",
    "has_path = bool(input_file_path)\n",
    "has_url = bool(input_url)\n",
    "assert (has_upload + has_path + has_url) == 1, \"Provide exactly one source (upload OR file path OR URL).\"\n",
    "\n",
    "sources = []\n",
    "if has_url:\n",
    "    sources.append({\"kind\": \"http\", \"url\": input_url})\n",
    "else:\n",
    "    if has_path:\n",
    "        with open(input_file_path, \"rb\") as f:\n",
    "            b = f.read()\n",
    "        filename = os.path.basename(input_file_path)\n",
    "    else:  # has_upload\n",
    "        b = uploaded_bytes\n",
    "        filename = \"upload.bin\"\n",
    "    sources.append({\n",
    "        \"kind\": \"file\",\n",
    "        \"base64_string\": base64.b64encode(b).decode(\"ascii\"),\n",
    "        \"filename\": filename,\n",
    "    })\n",
    "\n",
    "payload = {\n",
    "    \"sources\": sources,\n",
    "    \"chunking_options\": {\n",
    "        \"chunker\": \"hybrid\",\n",
    "        \"tokenizer\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"max_tokens\": TARGET_TOKENS,\n",
    "        \"merge_peers\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{DOCLING_BASE_URL}/v1/chunk/hybrid/source\", json=payload, timeout=300)\n",
    "resp.raise_for_status()\n",
    "result = resp.json()\n",
    "\n",
    "chunks = result.get(\"chunks\", [])\n",
    "chunk_texts = [c.get(\"text\", \"\") for c in chunks]\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Chunk Markdown using Docling (≈384 tokens per chunk)\n",
    "from typing import List\n",
    "\n",
    "# Try Docling chunker if available; otherwise simple token-based chunking\n",
    "try:\n",
    "    from docling.chunking import HierarchicalChunker\n",
    "    from docling.core.types import DoclingDocument\n",
    "    have_docling_chunker = True\n",
    "except Exception:\n",
    "    have_docling_chunker = False\n",
    "\n",
    "TARGET_TOKENS = 384  # all-MiniLM-L6-v2 embedding dimension; we approximate token length\n",
    "\n",
    "# Tokenizer: lightweight whitespace split for portability\n",
    "\n",
    "def split_into_token_chunks(text: str, max_tokens: int) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        if chunk_tokens:\n",
    "            chunks.append(\" \".join(chunk_tokens))\n",
    "    return chunks\n",
    "\n",
    "if have_docling_chunker and 'doc' in locals():\n",
    "    # Use Docling hierarchical chunker to get semantically coherent pieces first\n",
    "    chunker = HierarchicalChunker()\n",
    "    docling_chunks = list(chunker.chunk(doc))\n",
    "    base_segments = [c.text for c in docling_chunks if getattr(c, 'text', '')]\n",
    "    text_for_chunking = \"\\n\\n\".join(base_segments)\n",
    "else:\n",
    "    text_for_chunking = markdown_content or \"\"\n",
    "\n",
    "chunk_texts = split_into_token_chunks(text_for_chunking, TARGET_TOKENS)\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")\n",
    "print(\"--- First 10 chunks ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5de3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — Save chunks to Chroma (`file-explorer`) with default embeddings\n",
    "from chromadb import HttpClient\n",
    "\n",
    "chroma_client = HttpClient(host=\"localhost\", port=5002)\n",
    "collection = chroma_client.get_or_create_collection(name=\"file-explorer\")\n",
    "\n",
    "# Schema mirrors example: documents, metadatas, ids\n",
    "documents = chunk_texts\n",
    "metadatas = [\n",
    "    {\n",
    "        \"source\": input_file_path or input_url,\n",
    "        \"chunk_index\": i,\n",
    "        \"type\": \"docling_markdown_chunk\",\n",
    "    }\n",
    "    for i in range(len(documents))\n",
    "]\n",
    "ids = [f\"dl_chunk_{i+1}\" for i in range(len(documents))]\n",
    "\n",
    "# Use upsert to avoid duplicates on re-run\n",
    "collection.upsert(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "print(f\"Saved {len(documents)} chunks to Chroma collection 'file-explorer'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
